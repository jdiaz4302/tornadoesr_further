{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This is exploring and modifying PyTorch/Pyro's bayesian regression capabilities; heavily borrowing from their provided example notebook, available at: http://pyro.ai/examples/bayesian_regression.html\n",
    "\n",
    "I'm going to scale up the toy data set size, for learning purposes. Namely, the number of features (1 -> 10) and the number of data points (100 -> 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# 1. Regular regression part\n",
    "**Note: I've scaled this up to multiple variables (10), the original tutorial only had 1 X-variable**\n",
    "\n",
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Defining functions and parameters for creating an experimenting data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # size of toy data\n",
    "\n",
    "def build_linear_dataset(N, p = 10, noise_std=0.01):\n",
    "    X = np.random.rand(N, p)\n",
    "    # w = 3\n",
    "    w = 3 * np.ones(p)\n",
    "    # b = 1\n",
    "    y = np.matmul(X, w) + np.repeat(1, N) + np.random.normal(0, noise_std, size=N)\n",
    "    y = y.reshape(N, 1)\n",
    "    X, y = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "    data = torch.cat((X, y), 1)\n",
    "    assert data.shape == (N, p + 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Defining the simple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        # p = number of features\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Defining the training parameters & function, then training\n",
    "Also printing loss and learned parameters\n",
    "\n",
    "I bumped:\n",
    "1. the number of training epochs from 1000 -> 10000\n",
    "2. the printing freq from 50 -> 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0250] loss: 114.1863\n",
      "[iteration 0500] loss: 86.1122\n",
      "[iteration 0750] loss: 62.4182\n",
      "[iteration 1000] loss: 42.9902\n",
      "[iteration 1250] loss: 27.5911\n",
      "[iteration 1500] loss: 16.3051\n",
      "[iteration 1750] loss: 8.8034\n",
      "[iteration 2000] loss: 4.3176\n",
      "[iteration 2250] loss: 1.9235\n",
      "[iteration 2500] loss: 0.7948\n",
      "[iteration 2750] loss: 0.3308\n",
      "[iteration 3000] loss: 0.1670\n",
      "[iteration 3250] loss: 0.1183\n",
      "[iteration 3500] loss: 0.1063\n",
      "[iteration 3750] loss: 0.1039\n",
      "[iteration 4000] loss: 0.1036\n",
      "[iteration 4250] loss: 0.1035\n",
      "[iteration 4500] loss: 0.1035\n",
      "[iteration 4750] loss: 0.1035\n",
      "[iteration 5000] loss: 0.1035\n",
      "[iteration 5250] loss: 0.1035\n",
      "[iteration 5500] loss: 0.1035\n",
      "[iteration 5750] loss: 0.1035\n",
      "[iteration 6000] loss: 0.1035\n",
      "[iteration 6250] loss: 0.1035\n",
      "[iteration 6500] loss: 0.1035\n",
      "[iteration 6750] loss: 0.1035\n",
      "[iteration 7000] loss: 0.1035\n",
      "[iteration 7250] loss: 0.1035\n",
      "[iteration 7500] loss: 0.1035\n",
      "[iteration 7750] loss: 0.1035\n",
      "[iteration 8000] loss: 0.1035\n",
      "[iteration 8250] loss: 0.1035\n",
      "[iteration 8500] loss: 0.1035\n",
      "[iteration 8750] loss: 0.1035\n",
      "[iteration 9000] loss: 0.1035\n",
      "[iteration 9250] loss: 0.1035\n",
      "[iteration 9500] loss: 0.1035\n",
      "[iteration 9750] loss: 0.1035\n",
      "[iteration 10000] loss: 0.1035\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight: [[2.9984915 3.0008717 3.0001233 3.0000815 2.9985917 2.9993608 2.9995944\n",
      "  2.9977229 2.99891   3.0010066]]\n",
      "linear.bias: [1.0030445]\n"
     ]
    }
   ],
   "source": [
    "# Defining loss\n",
    "loss_fn = torch.nn.MSELoss(size_average = False)\n",
    "# Defining optimizer\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr = 0.05)\n",
    "# Defining training length\n",
    "num_iterations = 10000 if not smoke_test else 2\n",
    "\n",
    "# Defining function to train\n",
    "def main():\n",
    "    # Generating the data\n",
    "    data = build_linear_dataset(N)\n",
    "    # Defining X\n",
    "    x_data = data[:, :-1]\n",
    "    # Defining Y\n",
    "    y_data = data[:, -1]\n",
    "    # Training loop\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        # Print loss every 50 iterations\n",
    "        if (j + 1) % 250 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"\\nLearned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: \" % (name) + str(param.data.numpy()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true (generated) parameters were 3 and 1, with a noise of $\\sigma = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible typos in Pyro documentation**:\n",
    "\n",
    "1. They say that $\\sigma = 0.1$\n",
    "2. They say they used a learning rate of $0.01$\n",
    "2. They say they train for $500$ iterations\n",
    "\n",
    "None of the above seem to agree with their code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimally modified from Pyro's documentation:\n",
    "\n",
    "`random_module()` effectively takes a given `nn.Module` (from PyTorch) and turns it into a distribution over the same module; in our case, this will be a distribution over regressors. Specifically, each parameter in the original regression model is sampled from the provided prior. This allows us to repurpose vanilla regression models for use in the Bayesian setting.\n",
    "\n",
    "## Toy example - simpler than the next header's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a standard normal distribution prior\n",
    "# We need to feed the distribution tensors, rather than ints/floats\n",
    "loc = torch.zeros(1, 1)\n",
    "scale = torch.ones(1, 1)\n",
    "# Recall that \"Normal\" is from pyro.distributions\n",
    "prior = Normal(loc, scale)\n",
    "\n",
    "# overload the parameters in the regression module with samples from the prior\n",
    "Bayesian_Model = pyro.random_module(\"abitrary_name\",\n",
    "                                     regression_model,     # The PyTorch model\n",
    "                                     prior)                # The Pyro prior\n",
    "# sample a regressor from the prior\n",
    "sampled_reg_model = Bayesian_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Defining the model, which we will train\n",
    "Another possible code comment typo: their comments say unit normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    \n",
    "    \n",
    "    # Create wide normal priors - mu = 0, sigma = 10\n",
    "    weights_loc, weights_scale = torch.zeros(1, 10), (10 * torch.ones(1, 10))\n",
    "    # For the bias, defining mu and sigma\n",
    "    bias_loc, bias_scale = torch.zeros(1), 10 * torch.ones(1)\n",
    "    \n",
    "    \n",
    "    # For the weight, creating and assigning the prior distribution\n",
    "    w_priors = Normal(weights_loc, weights_scale).independent(1)\n",
    "    # For the bias, creating and assigning the prior distribution\n",
    "    b_prior = Normal(bias_loc, bias_scale).independent(1)\n",
    "    \n",
    "    \n",
    "    # Putting those into a dictionary\n",
    "    priors = {'linear.weights': w_priors,\n",
    "              'linear.bias': b_prior}\n",
    "    \n",
    "    \n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    # This is like creating the nn.Module (analogy to PyTorch)\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    # This is like calling the nn.Module (analogy to PyTorch)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    \n",
    "    with pyro.iarange(\"map\", N):\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "\n",
    "        # run the regressor forward conditioned on data\n",
    "        prediction_mean = lifted_reg_model(x_data).squeeze(-1)\n",
    "        \n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\",\n",
    "                    Normal(prediction_mean, 0.1 * torch.ones(data.size(0))),\n",
    "                    obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guide serves the purpose of having all the trainable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(data):\n",
    "    \n",
    "    \n",
    "    # define our variational parameters\n",
    "    w_locs = torch.randn(1, 10)\n",
    "    # note that we initialize our scales to be pretty narrow\n",
    "    w_log_sigs = torch.tensor(-3.0 * torch.ones(1, 10) + 0.05 * torch.randn(1, 10))\n",
    "    b_loc = torch.randn(1, 1)\n",
    "    b_log_sig = torch.tensor(-3.0 * torch.ones(1, 1) + 0.05 * torch.randn(1, 1))\n",
    "    \n",
    "    \n",
    "    # register learnable params in the param store\n",
    "    mw_params = pyro.param(\"guide_mean_weights\", w_locs)\n",
    "    sw_params = softplus(pyro.param(\"guide_log_scale_weights\", w_log_sigs))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_loc)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_scale_bias\", b_log_sig))\n",
    "    \n",
    "    # guide distributions for w and b\n",
    "    w_dists = Normal(mw_params, sw_params).independent(1)\n",
    "    b_dist = Normal(mb_param, sb_param).independent(1)\n",
    "    dists = {'linear.weights': w_dists,\n",
    "             'linear.bias': b_dist}\n",
    "    \n",
    "    \n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.05})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1.000000] loss: 8.5390\n",
      "[iteration 251.000000] loss: -1.3000\n",
      "[iteration 501.000000] loss: -1.3423\n",
      "[iteration 751.000000] loss: -1.3663\n",
      "[iteration 1001.000000] loss: -1.3719\n",
      "[iteration 1251.000000] loss: -1.3253\n",
      "[iteration 1501.000000] loss: -1.3678\n",
      "[iteration 1751.000000] loss: -1.3617\n",
      "[iteration 2001.000000] loss: -1.3683\n",
      "[iteration 2251.000000] loss: -1.2813\n",
      "[iteration 2501.000000] loss: -1.3698\n",
      "[iteration 2751.000000] loss: -1.3619\n",
      "[iteration 3001.000000] loss: -1.3578\n",
      "[iteration 3251.000000] loss: -1.3544\n",
      "[iteration 3501.000000] loss: -1.3675\n",
      "[iteration 3751.000000] loss: -1.3672\n",
      "[iteration 4001.000000] loss: -1.3683\n",
      "[iteration 4251.000000] loss: -1.3204\n",
      "[iteration 4501.000000] loss: -1.3286\n",
      "[iteration 4751.000000] loss: -1.3659\n",
      "[iteration 5001.000000] loss: -1.3699\n",
      "[iteration 5251.000000] loss: -1.3693\n",
      "[iteration 5501.000000] loss: -1.3697\n",
      "[iteration 5751.000000] loss: -1.3693\n",
      "[iteration 6001.000000] loss: -1.3174\n",
      "[iteration 6251.000000] loss: -1.3700\n",
      "[iteration 6501.000000] loss: -1.3610\n",
      "[iteration 6751.000000] loss: -1.3691\n",
      "[iteration 7001.000000] loss: -1.3686\n",
      "[iteration 7251.000000] loss: -1.3705\n",
      "[iteration 7501.000000] loss: -1.3714\n",
      "[iteration 7751.000000] loss: -1.3686\n",
      "[iteration 8001.000000] loss: -1.3699\n",
      "[iteration 8251.000000] loss: -1.3242\n",
      "[iteration 8501.000000] loss: -1.3694\n",
      "[iteration 8751.000000] loss: -1.3668\n",
      "[iteration 9001.000000] loss: -1.3489\n",
      "[iteration 9251.000000] loss: -1.3696\n",
      "[iteration 9501.000000] loss: -1.3650\n",
      "[iteration 9751.000000] loss: -1.3494\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    data = build_linear_dataset(N)\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if (j) % 250 == 0:\n",
    "            print(\"[iteration %04f] loss: %.4f\" % ((j+1), loss / float(N)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[guide_mean_weights]: [[ 0.9496766   0.03266967  0.6193119  -3.3383305  -0.53291076 -0.4834933\n",
      "   0.0514564  -1.1796219  -0.01656455 -0.18051535]]\n",
      "\n",
      "[guide_log_scale_weights]: [[-3.0165079 -2.9443128 -3.0165148 -2.9527166 -2.9659028 -3.0106964\n",
      "  -3.0520527 -3.0153282 -2.9680774 -3.018161 ]]\n",
      "\n",
      "[guide_mean_bias]: [[1.0038915]]\n",
      "\n",
      "[guide_log_scale_bias]: [[-5.7794676]]\n",
      "\n",
      "[module$$$linear.weight]: [[3.0055509 3.0038602 3.0039666 3.0029285 3.0065365 3.0043755 3.003794\n",
      "  3.0049517 3.0039704 3.0012436]]\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"\\n[%s]: \" % (name) + str(pyro.param(name).data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.07375814020633698\n"
     ]
    }
   ],
   "source": [
    "one_x = np.linspace(6, 7, num = 20)\n",
    "Xs = np.repeat(one_x[:, np.newaxis], 10, axis = 1)\n",
    "\n",
    "\n",
    "y =  np.sum(3*Xs, axis = 1) + 1\n",
    "y = y.reshape((20, 1))\n",
    "\n",
    "\n",
    "x_data, y_data = torch.tensor(Xs).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "   \n",
    "    \n",
    "y_preds = torch.zeros(20, 1)\n",
    "\n",
    "\n",
    "num_samples_of_param = 100\n",
    "for i in range(num_samples_of_param):\n",
    "    # guide does not require the data\n",
    "    sampled_reg_model = guide(None)\n",
    "    \n",
    "    # run the regression model and add prediction to total\n",
    "    y_preds = y_preds + sampled_reg_model(x_data)\n",
    "# take the average of the predictions\n",
    "y_preds = y_preds / num_samples_of_param\n",
    "print (\"Loss: \", loss(y_preds, y_data).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Part 3. Bayesian with Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        # p = number of features\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(p, p)\n",
    "        self.linear2 = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        first_layer_out = self.linear1(x)\n",
    "        second_layer_out = self.linear2(x)\n",
    "        return(second_layer_out)\n",
    "\n",
    "feedforward_nn = FeedForwardNN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    \n",
    "    \n",
    "    # First layer\n",
    "    #    We have 10 input variables that go to 10 hidden units (10*10)\n",
    "    weights_loc_first, weights_scale_first = torch.zeros(1, 10*10), (10 * torch.ones(1, 10*10))\n",
    "    #    We have 1 intercept per hidden unit (1*10)\n",
    "    bias_loc_first, bias_scale_first = torch.zeros(1, 1*10), 10 * torch.ones(1, 1*10)\n",
    "    \n",
    "    # Second layer\n",
    "    #    We have 10 hidden activations that go to 1 output unit\n",
    "    weights_loc_second, weights_scale_second = torch.zeros(1, 1*10), (10 * torch.ones(1, 1*10))\n",
    "    #    We have 1 intercept per hidden unit (1*1)\n",
    "    bias_loc_second, bias_scale_second = torch.zeros(1, 1*1), 10 * torch.ones(1, 1*1)\n",
    "    \n",
    "    \n",
    "    # For the first layer\n",
    "    #    Creating and assigning the prior distributions for the weights\n",
    "    w_priors_first = Normal(weights_loc_first, weights_scale_first).independent(1)\n",
    "    #    Creating and assigning the prior distribution for the intercepts\n",
    "    b_prior_first = Normal(bias_loc_first, bias_scale_first).independent(1)\n",
    "    \n",
    "    # For the second layer\n",
    "    #    Creating and assiging the prior distributions for the weights\n",
    "    w_priors_second = Normal(weights_loc_second, weights_scale_second).independent(1)\n",
    "    #    Creating and assigning the prior distribution for the intercepts\n",
    "    b_prior_second = Normal(bias_loc_second, bias_scale_second).independent(1)\n",
    "    \n",
    "    \n",
    "    # Putting those into a dictionary\n",
    "    priors = {'linear.weights_first': w_priors_first,\n",
    "              'linear.bias_first': b_prior_first,\n",
    "              'linear.weights_second': w_priors_second,\n",
    "              'linear.bias_second': b_prior_second}\n",
    "    \n",
    "    \n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    # This is like creating the nn.Module (analogy to PyTorch)\n",
    "    lifted_module = pyro.random_module(\"module\", feedforward_nn, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    # This is like calling the nn.Module (analogy to PyTorch)\n",
    "    lifted_nn_model = lifted_module()\n",
    "    \n",
    "    \n",
    "    with pyro.iarange(\"map\", N):\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "\n",
    "        # run the regressor forward conditioned on data\n",
    "        prediction_mean = lifted_nn_model(x_data).squeeze(-1)\n",
    "        \n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\",\n",
    "                    Normal(prediction_mean, 0.1 * torch.ones(data.size(0))),\n",
    "                    obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(data):\n",
    "    \n",
    "    \n",
    "    # First layer\n",
    "    #     The weights\n",
    "    w_locs_first = torch.randn(1, 10*10)\n",
    "    w_log_sigs_first = torch.tensor(-3.0 * torch.ones(1, 10*10) + 0.05 * torch.randn(1, 10*10))\n",
    "    #     The intercepts \n",
    "    b_loc_first = torch.randn(1, 1*10)\n",
    "    b_log_sig_first = torch.tensor(-3.0 * torch.ones(1, 1*10) + 0.05 * torch.randn(1, 1*10))\n",
    "    \n",
    "    # Second layer\n",
    "    #     The weights\n",
    "    w_locs_second = torch.randn(1, 10*10)\n",
    "    w_log_sigs_second = torch.tensor(-3.0 * torch.ones(1, 10*10) + 0.05 * torch.randn(1, 10*10))\n",
    "    #     The intercepts \n",
    "    b_loc_second = torch.randn(1, 1*10)\n",
    "    b_log_sig_second = torch.tensor(-3.0 * torch.ones(1, 1*10) + 0.05 * torch.randn(1, 1*10))\n",
    "    \n",
    "    \n",
    "    # Register learnable params in the param store\n",
    "    # First layer\n",
    "    #    The weights\n",
    "    mw_params_first = pyro.param(\"guide_mean_weights_first\", w_locs_first)\n",
    "    sw_params_first = softplus(pyro.param(\"guide_log_scale_weights_first\", w_log_sigs_first))\n",
    "    #    The intercepts\n",
    "    mb_param_first = pyro.param(\"guide_mean_bias_first\", b_loc_first)\n",
    "    sb_param_first = softplus(pyro.param(\"guide_log_scale_bias_first\", b_log_sig_first))\n",
    "    \n",
    "    # Second layer\n",
    "    #    The weights\n",
    "    mw_params_second = pyro.param(\"guide_mean_weights_second\", w_locs_second)\n",
    "    sw_params_second = softplus(pyro.param(\"guide_log_scale_weights_second\", w_log_sigs_second))\n",
    "    #    The intercepts\n",
    "    mb_param_second = pyro.param(\"guide_mean_bias_second\", b_loc_second)\n",
    "    sb_param_second = softplus(pyro.param(\"guide_log_scale_bias_second\", b_log_sig_second))\n",
    "    \n",
    "    \n",
    "    # guide distributions for w and b\n",
    "    # First layer\n",
    "    w_dists_first = Normal(mw_params_first, sw_params_first).independent(1)\n",
    "    b_dist_first = Normal(mb_param_first, sb_param_first).independent(1)\n",
    "    # Second layer\n",
    "    w_dists_second = Normal(mw_params_second, sw_params_second).independent(1)\n",
    "    b_dist_second = Normal(mb_param_second, sb_param_second).independent(1)\n",
    "    dists = {'linear.weights_first': w_dists_first,\n",
    "             'linear.bias_first': b_dist_first,\n",
    "             'linear.weights_second': w_dists_second,\n",
    "             'linear.bias_second': b_dist_second}\n",
    "    \n",
    "    \n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", feedforward_nn, dists)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.05})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 250.000000] loss: 2.9096\n",
      "[iteration 500.000000] loss: 1.9006\n",
      "[iteration 750.000000] loss: 1.0141\n",
      "[iteration 1000.000000] loss: 0.2720\n",
      "[iteration 1250.000000] loss: -0.3197\n",
      "[iteration 1500.000000] loss: -0.7541\n",
      "[iteration 1750.000000] loss: -1.0430\n",
      "[iteration 2000.000000] loss: -1.2159\n",
      "[iteration 2250.000000] loss: -1.3082\n",
      "[iteration 2500.000000] loss: -1.3518\n",
      "[iteration 2750.000000] loss: -1.3697\n",
      "[iteration 3000.000000] loss: -1.3760\n",
      "[iteration 3250.000000] loss: -1.3779\n",
      "[iteration 3500.000000] loss: -1.3784\n",
      "[iteration 3750.000000] loss: -1.3785\n",
      "[iteration 4000.000000] loss: -1.3785\n",
      "[iteration 4250.000000] loss: -1.3785\n",
      "[iteration 4500.000000] loss: -1.3785\n",
      "[iteration 4750.000000] loss: -1.3785\n",
      "[iteration 5000.000000] loss: -1.3785\n",
      "[iteration 5250.000000] loss: -1.3785\n",
      "[iteration 5500.000000] loss: -1.3785\n",
      "[iteration 5750.000000] loss: -1.3785\n",
      "[iteration 6000.000000] loss: -1.3785\n",
      "[iteration 6250.000000] loss: -1.3785\n",
      "[iteration 6500.000000] loss: -1.3785\n",
      "[iteration 6750.000000] loss: -1.3785\n",
      "[iteration 7000.000000] loss: -1.3785\n",
      "[iteration 7250.000000] loss: -1.3785\n",
      "[iteration 7500.000000] loss: -1.3785\n",
      "[iteration 7750.000000] loss: -1.3785\n",
      "[iteration 8000.000000] loss: -1.3785\n",
      "[iteration 8250.000000] loss: -1.3785\n",
      "[iteration 8500.000000] loss: -1.3785\n",
      "[iteration 8750.000000] loss: -1.3785\n",
      "[iteration 9000.000000] loss: -1.3785\n",
      "[iteration 9250.000000] loss: -1.3785\n",
      "[iteration 9500.000000] loss: -1.3785\n",
      "[iteration 9750.000000] loss: -1.3785\n",
      "[iteration 10000.000000] loss: -1.3785\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    data = build_linear_dataset(N)\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if (j + 1) % 250 == 0:\n",
    "            print(\"[iteration %04f] loss: %.4f\" % ((j+1), loss / float(N)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[guide_mean_weights_first]: [[ 0.1350662  -0.71258014  0.56413245 -1.0877305  -0.71924245 -0.18485934\n",
      "   0.08444668 -0.62668455 -1.5295157   0.14656499 -0.80894274  0.16689745\n",
      "  -0.38625666 -0.648746    0.7074361  -0.30794692 -0.7600678   0.4224629\n",
      "   0.18932785 -2.457124   -0.01532408 -0.76843655 -0.15489753 -2.3491802\n",
      "   1.2575742  -1.3192716   0.94393873  0.6760205   0.5581235   0.88240635\n",
      "   0.4687971  -1.9414772  -0.7755105  -0.6907118   1.1868346   0.3619952\n",
      "  -1.7380005   0.8275028  -1.0032492  -0.01862479 -0.14826553 -0.3772825\n",
      "   1.8741345  -0.78002036  0.97561646 -0.8535137  -0.5274611  -0.36480266\n",
      "   0.5219727   1.2725877  -0.30370033  0.992638    0.22331089 -1.213343\n",
      "   0.4647048  -1.3845404  -0.66757077 -0.9428688   0.66205996  0.17044587\n",
      "   0.52112764 -0.05161944 -0.34483764  1.6823118  -1.551885    0.21137127\n",
      "   0.07317776  0.78594923  0.05508348  1.5101274  -0.6225961  -0.3489961\n",
      "   0.30410987  0.6962635  -1.2059362  -1.5425351  -1.6297977  -0.8904026\n",
      "  -0.5457716  -0.18669009 -0.29438606  2.224922    0.30241933 -0.8642679\n",
      "   0.873401   -0.0561149  -0.9634523   1.0106077   1.601636    0.23172294\n",
      "   0.168588   -0.62666506  0.18121466  2.3200018   0.12740494 -1.8069876\n",
      "  -3.1429381  -0.8805113   0.09273842  0.5389131 ]]\n",
      "\n",
      "[guide_log_scale_weights_first]: [[-3.0128708 -3.041601  -2.951573  -3.024557  -3.0410707 -2.9748492\n",
      "  -2.9553187 -3.0854309 -2.9779234 -3.0782776 -3.0191576 -3.0415585\n",
      "  -2.9733505 -3.0111668 -3.1145914 -2.9398181 -3.0892062 -3.0891702\n",
      "  -2.9973588 -2.9687192 -3.0017765 -2.949563  -3.061696  -2.9094493\n",
      "  -2.994015  -3.0119767 -3.0611672 -3.0107079 -2.912407  -3.0060856\n",
      "  -3.0195694 -3.0501258 -3.0602117 -2.9172232 -3.049844  -3.0640986\n",
      "  -2.9506826 -3.0753436 -3.0678027 -3.0239284 -2.995055  -2.9984856\n",
      "  -3.035361  -2.9551036 -2.951915  -3.0114427 -2.956607  -2.8494706\n",
      "  -3.0322585 -3.115238  -3.0123868 -2.979217  -2.98169   -3.058937\n",
      "  -2.9287894 -2.9730916 -2.9309404 -2.9353046 -3.057502  -2.9760754\n",
      "  -3.0668566 -3.0207417 -3.0045571 -3.0352414 -3.0538235 -3.0283315\n",
      "  -2.9988508 -3.0144565 -2.9987342 -3.0207808 -3.0414531 -2.9445574\n",
      "  -3.027954  -3.0573323 -3.0016985 -2.9865956 -2.9699304 -3.060055\n",
      "  -3.0014117 -2.9910696 -2.971267  -3.0451996 -3.0622513 -3.083516\n",
      "  -3.0178664 -2.986787  -3.0444515 -3.0318768 -2.935591  -2.9825754\n",
      "  -3.0946658 -2.9584637 -3.0144303 -2.9575605 -2.947329  -2.997235\n",
      "  -3.01864   -3.0495398 -3.0241318 -3.0246227]]\n",
      "\n",
      "[guide_mean_bias_first]: [[-0.06679832  1.1124306   0.5005261   0.8857633   0.58784753 -0.6281237\n",
      "   0.41340792 -0.461157    1.3247954   0.22657841]]\n",
      "\n",
      "[guide_log_scale_bias_first]: [[-3.0313857 -2.924795  -2.989716  -3.0036433 -2.9788508 -2.9300818\n",
      "  -2.9813218 -3.0124483 -3.0777802 -2.9578722]]\n",
      "\n",
      "[guide_mean_weights_second]: [[-1.7805507  -0.12356427  1.1101719   0.7483005   1.7645135  -1.0869514\n",
      "   0.49526685  1.67681    -0.08291754 -1.9787979  -1.9276607   1.7607925\n",
      "   1.0781596   2.2381046   0.9625195   0.77937156  0.9949133   0.218243\n",
      "  -0.715379    1.1722337  -0.6766304   2.0301316  -0.5622721  -1.0742544\n",
      "  -0.5250836  -0.0151324   0.4123092   0.18217552 -1.4553305  -0.7642058\n",
      "   0.3751142  -0.21724747  1.110455    0.8176864  -0.12498128  0.29369235\n",
      "   0.558805   -1.8140528  -1.4608464   1.0773245   0.03155497 -2.6803887\n",
      "  -0.90291303 -0.96779066 -1.5408597   0.00715782 -0.14579645  0.42813018\n",
      "   0.17079073 -0.13984562  0.55800205  0.30317584  0.51739556  1.3811781\n",
      "  -0.516114   -1.8342396   0.25347912  2.1690485  -0.6667044   0.6721769\n",
      "   1.3461372   0.23897998 -0.6991011   0.15081157  0.58496034  0.7863772\n",
      "   2.0442033   0.63055867  0.9448131  -2.5412896  -0.8597439  -0.5702902\n",
      "   0.13301532  1.472455   -0.72906744 -1.3321004  -0.4602208  -1.0086426\n",
      "  -1.8001535  -2.0530171  -0.85732883 -0.11809888 -1.7328795   1.2848897\n",
      "  -0.8975134   0.31087726  0.10421849 -0.31655917  1.4290513  -0.10915259\n",
      "  -1.3805071  -0.6568447  -2.1469238   0.56206405  1.8128077  -0.21556376\n",
      "   0.6569469  -0.84563506 -1.6944375  -0.31514528]]\n",
      "\n",
      "[guide_log_scale_weights_second]: [[-2.9741724 -3.02682   -3.037107  -3.1277516 -2.9231973 -2.873411\n",
      "  -3.0174959 -3.0679777 -2.988129  -2.9389284 -2.8619995 -3.021919\n",
      "  -3.0114715 -2.9669664 -2.9828951 -2.97338   -3.0691602 -3.05871\n",
      "  -2.901447  -2.9975727 -3.0605068 -3.102928  -3.0065494 -3.0241013\n",
      "  -3.0528808 -2.9725542 -2.9533002 -3.0840168 -2.8691494 -2.9958508\n",
      "  -3.073915  -2.9778554 -3.0394416 -3.0342557 -2.8693428 -2.9414747\n",
      "  -3.008992  -3.0515907 -2.937096  -3.0726225 -3.011133  -3.0328314\n",
      "  -2.9638236 -3.0651505 -3.0259457 -2.9748058 -3.0466108 -3.0277798\n",
      "  -3.08641   -3.0508344 -3.0090332 -3.097883  -2.9177032 -3.0539358\n",
      "  -3.070703  -3.0160139 -3.0229998 -3.0727425 -2.9596865 -3.006865\n",
      "  -3.0346246 -2.935843  -2.9330711 -2.951534  -3.0176632 -2.9884167\n",
      "  -2.9753597 -3.0559978 -2.9702766 -2.9176602 -3.0148582 -2.9471655\n",
      "  -3.0538242 -2.948023  -2.9668844 -2.9538429 -3.0042582 -2.9395478\n",
      "  -2.9892557 -2.9748056 -3.0863647 -3.0101478 -3.0385184 -2.9241934\n",
      "  -3.0272713 -3.0788763 -2.9667983 -2.9749582 -3.0101156 -3.0170012\n",
      "  -3.1067686 -2.9918604 -2.9995303 -2.994273  -3.078781  -3.0316885\n",
      "  -3.0963771 -3.030596  -3.0525131 -3.0361967]]\n",
      "\n",
      "[guide_mean_bias_second]: [[ 1.5923278  -0.18603547 -0.46178395  0.8965089   0.1223282   0.2830332\n",
      "  -1.5844665   0.37866363  0.1506212   0.05427363]]\n",
      "\n",
      "[guide_log_scale_bias_second]: [[-3.0288982 -3.0047338 -3.0550277 -3.0718231 -3.0376365 -2.9326081\n",
      "  -2.9962409 -2.9745457 -3.0334244 -3.0217714]]\n",
      "\n",
      "[module$$$linear1.weight]: [[ 0.19242439 -0.21406439 -0.3009848  -0.0926678  -0.25879455 -0.27247956\n",
      "   0.14051497 -0.19528379  0.01453346 -0.2426545 ]\n",
      " [-0.09820694 -0.2322122  -0.26437372 -0.19574407  0.1314027  -0.02577186\n",
      "   0.29494843 -0.15965684  0.09267795 -0.06092063]\n",
      " [ 0.181548    0.07534963  0.08265767 -0.14383087  0.08460516 -0.25810224\n",
      "   0.09684825 -0.04189622  0.19714686 -0.05208054]\n",
      " [ 0.01962894 -0.0991635   0.10728505  0.12545148  0.08259526  0.13811201\n",
      "  -0.07997397 -0.1285265   0.11786905 -0.05876949]\n",
      " [ 0.10234368  0.14738226 -0.13400549 -0.2590531  -0.01936027  0.10879907\n",
      "  -0.20454112  0.00308776  0.01516205 -0.2188326 ]\n",
      " [-0.253356   -0.05313    -0.13574469 -0.12138674 -0.1589844  -0.2023611\n",
      "  -0.08214992 -0.24865775 -0.31604865 -0.06018123]\n",
      " [-0.12592764  0.0160186   0.03132021 -0.11414079 -0.19443914  0.2028127\n",
      "  -0.15712415 -0.0826446  -0.0479885   0.17211786]\n",
      " [-0.08357497  0.14250588  0.20150408 -0.1762851  -0.03812489  0.04817152\n",
      "  -0.18099742 -0.236388    0.05225843  0.14261007]\n",
      " [-0.11380257 -0.15418239 -0.06629592  0.1449219  -0.29817197 -0.08845337\n",
      "  -0.24310872  0.27098545  0.19656757  0.2659804 ]\n",
      " [ 0.24707839 -0.24215214 -0.19090913 -0.22522345 -0.02271503 -0.1894229\n",
      "  -0.21210854 -0.3106668  -0.14592937 -0.00088868]]\n",
      "\n",
      "[module$$$linear1.bias]: [-0.2963414  -0.10433124  0.13438249  0.24026647 -0.24547437 -0.11266991\n",
      " -0.25112846 -0.27159542 -0.13386232 -0.21031046]\n",
      "\n",
      "[module$$$linear2.weight]: [[2.9987426 2.9991555 3.0009768 2.999588  3.0000699 3.0022707 3.0015779\n",
      "  3.000252  2.9990447 2.998268 ]]\n",
      "\n",
      "[module$$$linear2.bias]: [0.9998986]\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"\\n[%s]: \" % (name) + str(pyro.param(name).data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.251821626941819e-07\n"
     ]
    }
   ],
   "source": [
    "one_x = np.linspace(6, 7, num = 20)\n",
    "Xs = np.repeat(one_x[:, np.newaxis], 10, axis = 1)\n",
    "\n",
    "\n",
    "y =  np.sum(3*Xs, axis = 1) + 1\n",
    "y = y.reshape((20, 1))\n",
    "\n",
    "\n",
    "x_data, y_data = torch.tensor(Xs).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "   \n",
    "    \n",
    "y_preds = torch.zeros(20, 1)\n",
    "\n",
    "\n",
    "num_samples_of_param = 100\n",
    "for i in range(num_samples_of_param):\n",
    "    # guide does not require the data\n",
    "    sampled_reg_model = guide(None)\n",
    "    \n",
    "    # run the regression model and add prediction to total\n",
    "    y_preds = y_preds + sampled_reg_model(x_data)\n",
    "# take the average of the predictions\n",
    "y_preds = y_preds / num_samples_of_param\n",
    "print (\"Loss: \", loss(y_preds, y_data).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
